# -*- coding: utf-8 -*-
"""triolysis_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ciA7dOwHSPZY81TPwGossFrXQXwm123k
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import KFold
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import shap

# 1. Load main dataset (includes income and geographic fields)
df_main = pd.read_excel("LTF Challenge data with dictionary.xlsx")

# 2. Load crop features (yields, zones, soil, water bodies, etc.)
df_crop_features = pd.read_csv("Crops_data.csv")

# 3. Load Agmarknet modal price dataset
df_prices_agmarknet = pd.read_csv("agmarknet_crop_prices.csv")

# 4. Load daily rainfall dataset
df_rainfall_district = pd.read_csv("daily-rainfall-data-district-level.csv")

# Merging LTF Challenge data with dictionary.xlsx and Crops_data.csv datasets
# Print Column Names
print("Main Dataset Columns:")
print(df_main.columns.tolist())

print("\nCrop Feature Dataset Columns:")
print(df_crop_features.columns.tolist())

# Merge crop dataset with main
merged_1 = df_main.merge(
    df_crop_features,
    left_on=['State', 'DISTRICT'],
    right_on=['State Name', 'Dist Name'],
    how='left'
)
merged_1.drop(columns=['State Name', 'Dist Name'], inplace=True)

# Save after merge
merged_1.to_csv("merged_1.csv", index=False)

# Load the merged_1 and the agmarket dataset
merged_1 = pd.read_csv("merged_1.csv")
df_prices = pd.read_csv("agmarknet_crop_prices.csv")

# Print Column Names for above datasets
print("Merged_1 Dataset Columns:")
print(merged_1.columns.tolist())

print("\nAgmarknet Price Dataset Columns:")
print(df_prices.columns.tolist())

# Clean column names
df_prices.columns = df_prices.columns.str.strip()

# Parse 'Arrival_Date' to datetime
df_prices['Arrival_Date'] = pd.to_datetime(df_prices['Arrival_Date'], errors='coerce')

# Extract year
df_prices['Year'] = df_prices['Arrival_Date'].dt.year

# Drop rows with null Year (bad date parse)
df_prices = df_prices.dropna(subset=['Year'])

# Convert Year to int (safer for merge keys)
df_prices['Year'] = df_prices['Year'].astype(int)

# Group and compute average modal price
df_prices_grouped = df_prices.groupby(
    ['State', 'District', 'Commodity']
)['Modal_x0020_Price'].mean().reset_index()

# Rename columns to match merged_1
df_prices_grouped.rename(columns={
    'District': 'DISTRICT',
    'Commodity': 'Crop',
    'Modal_x0020_Price': 'Avg_Modal_Price'
}, inplace=True)

# Merge on State and DISTRICT (since Crop is not present in merged_1)
merged_2 = merged_1.merge(
    df_prices_grouped,
    how='left',
    on=['State', 'DISTRICT']
)

merged_2.to_csv("merged_2.csv", index=False)

# Load  merged_2 and rainfall datasets
merged_2 = pd.read_csv("merged_2.csv")
df_rain = pd.read_csv("daily-rainfall-data-district-level.csv", engine='python')

# Print Column Names
print("Merged_2 Dataset Columns:")
print(merged_2.columns.tolist())

print("\nRainfall Dataset Columns:")
print(df_rain.columns.tolist())

# Clean column names
df_rain.columns = df_rain.columns.str.strip()

# Parse 'date' column to datetime
df_rain['date'] = pd.to_datetime(df_rain['date'], errors='coerce')

# Extract year
df_rain['Year'] = df_rain['date'].dt.year

# Drop invalid years (if any)
df_rain = df_rain.dropna(subset=['Year'])

# Convert Year to int
df_rain['Year'] = df_rain['Year'].astype(int)

# Rename state/district columns to match merged_2
df_rain.rename(columns={
    'state_name': 'State',
    'district_name': 'DISTRICT'
}, inplace=True)

# Aggregate rainfall features by State, DISTRICT, Year
df_rain_agg = df_rain.groupby(['State', 'DISTRICT', 'Year']).agg({
    'actual': 'mean',
    'normal': 'mean',
    'deviation': 'mean'
}).reset_index()

# Rename aggregated columns
df_rain_agg.rename(columns={
    'actual': 'avg_actual_rainfall',
    'normal': 'avg_normal_rainfall',
    'deviation': 'avg_rainfall_deviation'
}, inplace=True)

# Merge rainfall data into merged_2
merged_final = merged_2.merge(
    df_rain_agg,
    how='left',
    on=['State', 'DISTRICT', 'Year']
)

# Save the final merged dataset
merged_final.to_csv("merged_final.csv", index=False)

# Print shapes
print("Base/Main Dataset Shape:")
print("df_main:", df_main.shape)

print("\nCrop Feature Dataset Shape:")
print("df_crop_features:", df_crop_features.shape)

print("\nAgmarknet Price Dataset Shape:")
print("df_prices_agmarknet:", df_prices_agmarknet.shape)

print("\nRainfall Dataset Shape:")
print("df_rainfall_district:", df_rainfall_district.shape)

print("\nMerged Dataset After   1 (Main + Crop) Shape:")
print("merged_1:", merged_1.shape)

print("\nMerged Dataset After   2 (merged_1 + Agmarknet) Shape:")
print("merged_2:", merged_2.shape)

print("\nFinal Merged Dataset (merged_2 + Rainfall) Shape:")
print("merged_final:", merged_final.shape)

# Show only columns that have at least one missing value
null_columns = merged_final.isnull().sum()
null_columns = null_columns[null_columns > 0]

print("Columns with Missing Values:")
print(null_columns)

# Load the final merged dataset (Main + Crop + Price + Rainfall)
df = pd.read_csv("merged_final.csv")

# Columns to drop: IDs and codes that may leak information or are duplicated
columns_to_drop = [
    'FarmerID',        # Unique ID, not a predictive feature
    'Zipcode',         # Too granular, may cause overfitting
    'State Code',      # Already captured by 'State'
    'Dist Code'        # Already captured by 'DISTRICT'
]

# Drop unwanted columns from the DataFrame
df.drop(columns=columns_to_drop, errors='ignore', inplace=True)

# Separate the target variable for income prediction
target_col = 'Target_Variable/Total Income'

# Features (X): everything except the target
X = df.drop(columns=[target_col], errors='ignore')

# Target (y): the income column
y = df[target_col]

# Impute missing values in numeric columns with median
numeric_cols = X.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if X[col].isnull().any():
        X[col] = X[col].fillna(X[col].median())

# Impute missing values in categorical columns with 'Missing'
categorical_cols = X.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if X[col].isnull().any():
        X[col] = X[col].fillna('Missing')

# Final shape check after imputations
print("Final shape after handling missing values:", X.shape)

# Load the final merged dataset (Main + Crop + Price + Rainfall)
df = pd.read_csv("merged_final.csv")

# Columns to drop: IDs and codes that may leak information or are duplicated
columns_to_drop = [
    'FarmerID',        # Unique ID, not a predictive feature
    'Zipcode',         # Too granular, may cause overfitting
    'State Code',      # Already captured by 'State'
    'Dist Code'        # Already captured by 'DISTRICT'
]

# Drop unwanted columns from the DataFrame
df.drop(columns=columns_to_drop, errors='ignore', inplace=True)

# Separate the target variable for income prediction
target_col = 'Target_Variable/Total Income'

# Features (X): everything except the target
X = df.drop(columns=[target_col], errors='ignore')

# Target (y): the income column
y = df[target_col]

# Output the shape of features and target for confirmation
print(f"Cleaned feature matrix shape: {X.shape}")
print(f"Target variable shape: {y.shape}")
print("  2 completed: Leakage columns removed, target separated.\n")

# Feature Engineering — Rainfall Anomaly
# Create rainfall anomaly feature
# Difference between actual rainfall and normal rainfall
X['Rainfall_Anomaly'] = X['avg_actual_rainfall'] - X['avg_normal_rainfall']

# Confirm creation
print("Rainfall anomaly feature created as 'Rainfall_Anomaly'")

# Feature Engineering — Yield × Price
# Define the crops and their exact yield column names
crop_yield_mapping = {
    'RICE': 'RICE YIELD (Kg per ha)',
    'MAIZE': 'MAIZE YIELD (Kg per ha)',
    'WHEAT': 'WHEAT YIELD (Kg per ha)',
    'SUGARCANE': 'SUGARCANE YIELD (Kg per ha)',
    'SOYABEAN': 'SOYABEAN YIELD (Kg per ha)',
    'COTTON': 'COTTON YIELD (Kg per ha)',
    'GROUNDNUT': 'GROUNDNUT YIELD (Kg per ha)',
    'CHICKPEA': 'CHICKPEA YIELD (Kg per ha)',
    'PIGEONPEA': 'PIGEONPEA YIELD (Kg per ha)'
}

# Loop through each crop and create a value feature
for crop, yield_col in crop_yield_mapping.items():
    value_col = f"{crop}_Value"
    X[value_col] = X.apply(
        lambda row: row[yield_col] * row['Avg_Modal_Price'] if row['Crop'] == crop else 0,
        axis=1
    )

# Optional: Combine all crop values into a total economic value indicator
value_cols = [f"{crop}_Value" for crop in crop_yield_mapping.keys()]
X["Total_Crop_Value"] = X[value_cols].sum(axis=1)

# Confirm completion
print(f"Engineered crop value features: {', '.join(value_cols)}")
print("Created combined 'Total_Crop_Value' feature.")

# Feature Engineering — Crop Intensity Ratios
# Compute Kharif Crop Intensity
X['Kharif_Crop_Intensity'] = X['Kharif Seasons  Cropping density in 2022'] / (
    X['Kharif Seasons  Irrigated area in 2022'] + 1e-5)  # avoid division by zero

# Compute Rabi Crop Intensity
X['Rabi_Crop_Intensity'] = X['Rabi Seasons Cropping density in 2022'] / (
    X['Rabi Seasons  Season Irrigated area in 2022'] + 1e-5)

# Confirm creation
print("Created 'Kharif_Crop_Intensity' and 'Rabi_Crop_Intensity' features.")

# Feature Engineering — Binary Crop Flags
# Create binary indicator (1/0) if this row belongs to a specific crop
major_crops = ['RICE', 'MAIZE', 'WHEAT', 'SUGARCANE', 'SOYABEAN', 'COTTON', 'GROUNDNUT', 'CHICKPEA', 'PIGEONPEA']

for crop in major_crops:
    col_name = f"Is_{crop}"
    X[col_name] = (X['Crop'] == crop).astype(int)

# Confirm
print(f"Created binary flags: {', '.join(['Is_' + crop for crop in major_crops])}")

# Additional interaction features (can help reduce MAPE)
X["Value_per_mm_Rain"] = X["Total_Crop_Value"] / (X["avg_actual_rainfall"] + 1)
X["Rainfall_Intensity_Factor"] = X["Rainfall_Anomaly"] * X["Avg_Modal_Price"]

# Feature Selection — Agro Conditions (Soil/Water/Zone)

# Keep key agro-environmental features from Kharif and Rabi seasons
agro_cols = [
    'Kharif Seasons  Type of soil in 2022',
    'Kharif Seasons  Type of water bodies in hectares 2022',
    'Kharif Seasons  Agro Ecological Sub Zone in 2022',
    'Rabi Seasons Type of soil in 2022',
    'Rabi Seasons Type of water bodies in hectares 2022',
    'Rabi Seasons Agro Ecological Sub Zone in 2022'
]

# Print their null count to handle later
print("Null values in agro-environmental features:")
print(X[agro_cols].isnull().sum())

# These features can be encoded or used directly later

# Handling Missing Values — Identify First
# Show only columns with missing values
missing_cols = X.columns[X.isnull().any()]
print(f" Columns with missing values: {list(missing_cols)}")

# Optional: View counts
print("\n Missing Value Counts:")
print(X[missing_cols].isnull().sum())

# Safe Missing Value Imputation
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd

# Handle numeric columns
numeric_cols = X.select_dtypes(include=[np.number]).columns

# Drop numeric cols that are entirely missing (like Avg_Modal_Price)
numeric_cols_valid = [col for col in numeric_cols if X[col].isnull().sum() < len(X)]

# Impute only valid numeric cols
num_imputer = SimpleImputer(strategy='median')
X[numeric_cols_valid] = pd.DataFrame(
    num_imputer.fit_transform(X[numeric_cols_valid]),
    columns=numeric_cols_valid,
    index=X.index
)

# Handle categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

cat_imputer = SimpleImputer(strategy='constant', fill_value='Missing')
X[categorical_cols] = pd.DataFrame(
    cat_imputer.fit_transform(X[categorical_cols]),
    columns=categorical_cols,
    index=X.index
)

# Final check
print("Safe imputation complete.")
print("Remaining nulls (if any):", X.isnull().sum().sum())

import numpy as np

# Example: Cap outliers in income-related features (IQR method)
def cap_outliers_iqr(df, columns, k=1.5):
    df_capped = df.copy()
    for col in columns:
        Q1 = df_capped[col].quantile(0.25)
        Q3 = df_capped[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - k * IQR
        upper = Q3 + k * IQR
        df_capped[col] = np.where(
            df_capped[col] < lower, lower,
            np.where(df_capped[col] > upper, upper, df_capped[col])
        )
    return df_capped

# Apply outlier capping to key numeric columns
numeric_cols_to_cap = ['RICE YIELD (Kg per ha)', 'MAIZE YIELD (Kg per ha)', 'Avg_Modal_Price']

# Apply and overwrite X
X = cap_outliers_iqr(X, numeric_cols_to_cap)

print("Outliers capped using IQR method on selected columns.")

# Encode Categorical Columns — Label Encoding
from sklearn.preprocessing import LabelEncoder

# Identify object (categorical) columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Apply Label Encoding to each categorical column
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le  # Save encoder for test set later

print(f"Label encoded {len(categorical_cols)} categorical columns.")

from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from xgboost import XGBRegressor
import numpy as np
import pandas as pd
import re

# Ensure your target column exists
if 'Target_Variable/Total Income' in df.columns:
    y = df['Target_Variable/Total Income']
    y_log = np.log1p(y)
else:
    raise ValueError("Target column 'Target_Variable/Total Income' not found in dataframe.")

# Remove target from feature set if still present
X = df.drop(columns=['Target_Variable/Total Income'], errors='ignore')

# Missing Value Imputation for numeric columns (already done in AEpLRankZbun, but re-doing here for robustness in this combined step)
numeric_cols = X.select_dtypes(include=[np.number]).columns
numeric_cols_to_drop = [col for col in numeric_cols if X[col].isnull().all()]
X.drop(columns=numeric_cols_to_drop, inplace=True)
numeric_cols_after_drop = X.select_dtypes(include=[np.number]).columns

num_imputer = SimpleImputer(strategy='median')
X[numeric_cols_after_drop] = pd.DataFrame(
    num_imputer.fit_transform(X[numeric_cols_after_drop]),
    columns=numeric_cols_after_drop,
    index=X.index
)

# Impute missing values in categorical columns with 'Missing' (Crucial for consistent Label Encoding)
categorical_cols_before_encoding = X.select_dtypes(include=['object']).columns
cat_imputer = SimpleImputer(strategy='constant', fill_value='Missing')
X[categorical_cols_before_encoding] = pd.DataFrame(
    cat_imputer.fit_transform(X[categorical_cols_before_encoding]),
    columns=categorical_cols_before_encoding,
    index=X.index
)


# Label Encoding (Now done AFTER imputation of 'Missing')
categorical_cols_after_imputation = X.select_dtypes(include='object').columns
label_encoders = {}
for col in categorical_cols_after_imputation:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))  # ensure everything is string before encoding
    label_encoders[col] = le
print(f"Label encoded {len(categorical_cols_after_imputation)} categorical columns after imputation.")

# Clean column names again after transformations
X.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in X.columns]


# Drop weak features using feature importances
model = XGBRegressor(n_jobs=-1, random_state=42)
model.fit(X, y_log)

importance = model.feature_importances_
# Update low_value_features based on the current X columns
low_value_features = [X.columns[i] for i, imp in enumerate(importance) if imp < 0.001]

X.drop(columns=low_value_features, inplace=True)
print("Dropped weak features based on importance:", low_value_features)

# Set up cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

import numpy as np

# Log-transform the target variable
# Apply log1p transform for MAPE optimization
y_log = np.log1p(y)

# 4. Confirm
print(f"Log-transformed target created as 'y_log' — shape: {y_log.shape}")
print(f"Feature matrix (X) shape after drop: {X.shape}")

pip install catboost

from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.model_selection import KFold
from sklearn.metrics import (
    mean_absolute_percentage_error,
    mean_squared_error,
    mean_absolute_error,
    r2_score
)
import numpy as np
import re

# Set number of splits for K-Fold CV
N_SPLITS = 10

# Clean column names (once)
X.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in X.columns]

# Evaluation function
def evaluate_model(name, model, X, y_log, n_splits=N_SPLITS):
    print(f"\nTraining {name} with {n_splits}-Fold CV...")
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

    mape_scores, rmse_scores, mae_scores, r2_scores = [], [], [], []
    oof_preds = np.zeros(X.shape[0])

    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]

        model.fit(X_train, y_train)
        preds_log = model.predict(X_val)
        oof_preds[val_idx] = preds_log

        preds = np.expm1(preds_log)
        y_val_actual = np.expm1(y_val)

        mape = mean_absolute_percentage_error(y_val_actual, preds) * 100
        rmse = np.sqrt(mean_squared_error(y_val_actual, preds))
        mae = mean_absolute_error(y_val_actual, preds)
        r2 = r2_score(y_val_actual, preds)

        mape_scores.append(mape)
        rmse_scores.append(rmse)
        mae_scores.append(mae)
        r2_scores.append(r2)

        print(f"[{name}] Fold {fold} | MAPE: {mape:.4f}% | RMSE: {rmse:.2f} | MAE: {mae:.2f} | R²: {r2:.4f}")

    print(f"\n{name} Final CV Results ({n_splits}-Fold):")
    print(f"Average MAPE: {np.mean(mape_scores):.4f}%")
    print(f"Average RMSE: {np.mean(rmse_scores):.2f}")
    print(f"Average MAE: {np.mean(mae_scores):.2f}")
    print(f"Average R²: {np.mean(r2_scores):.4f}\n")

    return oof_preds, {
        "MAPE": np.mean(mape_scores),
        "RMSE": np.mean(rmse_scores),
        "MAE": np.mean(mae_scores),
        "R2": np.mean(r2_scores)
    }

# Model setups
lgbm_model = LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=7,
    random_state=42,
    n_jobs=-1
)

xgb_model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

cat_model = CatBoostRegressor(
    iterations=1000,
    learning_rate=0.05,
    depth=7,
    random_seed=42,
    verbose=0
)

# Run evaluations
lgbm_preds, lgbm_metrics = evaluate_model("LightGBM", lgbm_model, X, y_log)
xgb_preds, xgb_metrics = evaluate_model("XGBoost", xgb_model, X, y_log)
cat_preds, cat_metrics = evaluate_model("CatBoost", cat_model, X, y_log)

# Final model comparison
print("FINAL COMPARISON:")
print(f"LightGBM - MAPE: {lgbm_metrics['MAPE']:.4f}, RMSE: {lgbm_metrics['RMSE']:.2f}, R²: {lgbm_metrics['R2']:.4f}")
print(f"XGBoost  - MAPE: {xgb_metrics['MAPE']:.4f}, RMSE: {xgb_metrics['RMSE']:.2f}, R²: {xgb_metrics['R2']:.4f}")
print(f"CatBoost - MAPE: {cat_metrics['MAPE']:.4f}, RMSE: {cat_metrics['RMSE']:.2f}, R²: {cat_metrics['R2']:.4f}")

# Final model comparison
print("FINAL COMPARISON:")
print(f"LightGBM - MAPE: {lgbm_metrics['MAPE']:.4f}, RMSE: {lgbm_metrics['RMSE']:.2f}, R²: {lgbm_metrics['R2']:.4f}")
print(f"XGBoost  - MAPE: {xgb_metrics['MAPE']:.4f}, RMSE: {xgb_metrics['RMSE']:.2f}, R²: {xgb_metrics['R2']:.4f}")
print(f"CatBoost - MAPE: {cat_metrics['MAPE']:.4f}, RMSE: {cat_metrics['RMSE']:.2f}, R²: {cat_metrics['R2']:.4f}")

pip install optuna

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import pandas as pd
from xgboost import XGBRegressor

# Bin target variable for stratification
y_bins = pd.qcut(y, q=10, duplicates='drop').astype(str)

# Set up Stratified K-Fold
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Prepare tracking
oof_preds_log = np.zeros(len(X))

print("\nEvaluating Tuned XGBoost with Stratified 10-Fold CV:")
# Check if best_params is defined before proceeding
if 'best_params' not in globals():
    print("Error: 'best_params' is not defined. Please run the Optuna optimization cell first.")
else:
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_bins), 1):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]

        # Create a copy of best_params and remove 'random_state'
        model_params = best_params.copy()
        if 'random_state' in model_params:
            del model_params['random_state']

        model = XGBRegressor(**model_params, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)

        preds_log = model.predict(X_val)
        oof_preds_log[val_idx] = preds_log

        preds = np.expm1(preds_log)
        y_val_actual = np.expm1(y_val)

        mape = mean_absolute_percentage_error(y_val_actual, preds) * 100
        rmse = np.sqrt(mean_squared_error(y_val_actual, preds))
        mae = mean_absolute_error(y_val_actual, preds)
        r2 = r2_score(y_val_actual, preds)

        print(f"[Fold {fold}] MAPE: {mape:.4f}% | RMSE: {rmse:.2f} | MAE: {mae:.2f} | R²: {r2:.4f}")

    # Aggregate results from all folds
    final_preds = np.expm1(oof_preds_log)
    y_actual = np.expm1(y_log)

    final_mape = mean_absolute_percentage_error(y_actual, final_preds) * 100
    final_rmse = np.sqrt(mean_squared_error(y_actual, final_preds))
    final_mae = mean_absolute_error(y_actual, final_preds)
    final_r2 = r2_score(y_actual, final_preds)

    print("\nFinal Stratified 10-Fold CV Performance:")
    print(f"MAPE: {final_mape:.4f}")
    print(f"RMSE: {final_rmse:.2f}")
    print(f"MAE: {final_mae:.2f}")
    print(f"R²: {final_r2:.4f}")

import optuna
from xgboost import XGBRegressor
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import make_scorer, mean_absolute_percentage_error
import numpy as np

# Bin target for stratification
y_bins = pd.qcut(y, q=10, duplicates='drop').astype(str)

# Objective for Optuna
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),
        'random_state': 42,
        'n_jobs': -1
    }

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    mape_scores = []
    for train_idx, val_idx in skf.split(X, y_bins):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]

        model = XGBRegressor(**params)
        model.fit(X_train, y_train)

        preds = np.expm1(model.predict(X_val))
        y_val_actual = np.expm1(y_val)

        mape = mean_absolute_percentage_error(y_val_actual, preds)
        mape_scores.append(mape)

    return np.mean(mape_scores)

# Run the Optuna study
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50, timeout=1800)  # Try 50 trials or 30 minutes

# Get best params and value
best_params = study.best_params
print(f"\nBest MAPE: {study.best_value:.4f}")
print("Best Parameters:")
for k, v in best_params.items():
    print(f"{k}: {v}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from joblib import load
from xgboost import XGBRegressor
from sklearn.metrics import (
    mean_absolute_percentage_error,
    mean_squared_error,
    mean_absolute_error,
    r2_score
)

# Load the model and data
model = tuned_xgb_model

# Sample a subset for SHAP
subset_size = 5000
X_subset = X.sample(subset_size, random_state=42) if len(X) > subset_size else X

# SHAP TreeExplainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_subset)

# 1. SHAP Summary Bar Plot
plt.figure()
shap.summary_plot(shap_values, X_subset, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (Tuned XGBoost)")
plt.tight_layout()
plt.savefig("shap_summary_bar_tuned_xgb.png")
plt.show()

# 2. SHAP Beeswarm Plot
plt.figure()
shap.summary_plot(shap_values, X_subset, show=False)
plt.title("SHAP Beeswarm Plot (Tuned XGBoost)")
plt.tight_layout()
plt.savefig("shap_beeswarm_tuned_xgb.png")
plt.show()

# 3. SHAP Dependence Plot for top 1 feature
top_feature = X_subset.columns[np.argsort(np.abs(shap_values).mean(0))[-1]]
plt.figure()
shap.dependence_plot(top_feature, shap_values, X_subset, show=False)
plt.title(f"SHAP Dependence Plot: {top_feature}")
plt.tight_layout()
plt.savefig("shap_dependence_top_feature.png")
plt.show()

# 4. Target Income Distribution
plt.figure(figsize=(8, 4))
sns.histplot(np.expm1(y_log), bins=50, kde=True)
plt.title("Income Distribution")
plt.xlabel("Income")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig("income_distribution.png")
plt.show()

# 5. Scatter Plot: Crop Value vs Income
if 'Total_Crop_Value' in X.columns:
    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=X['Total_Crop_Value'], y=np.expm1(y_log), alpha=0.5)
    plt.title("Total Crop Value vs Income")
    plt.xlabel("Total Crop Value")
    plt.ylabel("Income")
    plt.tight_layout()
    plt.savefig("crop_value_vs_income.png")
    plt.show()

# 6. Correlation Heatmap of Top 15 SHAP Features
if len(X_subset.columns) >= 15:
    top_shap_indices = np.argsort(np.abs(shap_values).mean(0))[-15:]
    top_feature_names = X_subset.columns[top_shap_indices]
    plt.figure(figsize=(10, 8))
    sns.heatmap(X[top_feature_names].corr(), annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Top 15 SHAP Features - Correlation Heatmap")
    plt.tight_layout()
    plt.savefig("shap_top_corr_heatmap_tuned_xgb.png")
    plt.show()

# 7. Actual vs Predicted Income
full_preds_log = model.predict(X)
full_preds = np.expm1(full_preds_log)
y_actual = np.expm1(y_log)

plt.figure(figsize=(8, 6))
plt.scatter(y_actual, full_preds, alpha=0.5)
plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'k--', lw=2)
plt.title("Actual vs Predicted Income (Tuned XGBoost)")
plt.xlabel("Actual Income")
plt.ylabel("Predicted Income")
plt.tight_layout()
plt.savefig("actual_vs_predicted_income_tuned_xgb.png")
plt.show()

# 8. Residual Plot
residuals = y_actual - full_preds
plt.figure(figsize=(8, 5))
plt.scatter(full_preds, residuals, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residual Plot (Tuned XGBoost)")
plt.xlabel("Predicted Income")
plt.ylabel("Residual (Actual - Predicted)")
plt.tight_layout()
plt.savefig("residual_plot_tuned_xgb.png")
plt.show()

# Final Evaluation Metrics
final_mape = mean_absolute_percentage_error(y_actual, full_preds) * 100
final_rmse = np.sqrt(mean_squared_error(y_actual, full_preds))
final_mae = mean_absolute_error(y_actual, full_preds)
final_r2 = r2_score(y_actual, full_preds)

print("\nFinal Tuned XGBoost Model Performance (on full training data):")
print(f"MAPE: {final_mape:.4f}")
print(f"RMSE: {final_rmse:.2f}")
print(f"MAE: {final_mae:.2f}")
print(f"R²: {final_r2:.4f}")

import pandas as pd
import numpy as np
from joblib import load

# Use trained model (ensure it's defined earlier in your code)
# model = tuned_xgb_model # This line is not needed if tuned_xgb_model is already in the environment

# Predict and reverse log transformation
preds_log = tuned_xgb_model.predict(X) # Use the existing tuned_xgb_model
predictions = np.expm1(preds_log)

# Retrieve original FarmerID aligned with X
# Load the original main dataframe to get FarmerID
df_main_original = pd.read_excel("LTF Challenge data with dictionary.xlsx")

# Assuming the indices of df_main_original and X are aligned
farmer_ids = df_main_original.loc[X.index, "FarmerID"]


# Prepare final prediction DataFrame
submission_df = pd.DataFrame({
    "FarmerID": farmer_ids,
    "Target_Variable/Total Income": predictions
})

# Save to CSV in required format
submission_df.to_csv("Triolysis_predictions.csv", index=False)
print("Final prediction file saved as 'Triolysis_predictions.csv'")

from xgboost import XGBRegressor
from joblib import dump

# Train the final XGBoost model using the best parameters found by Optuna
# Ensure best_params is defined from the Optuna optimization step
if 'best_params' in globals():
    # Create a copy of best_params and remove 'random_state' if present
    model_params = best_params.copy()
    if 'random_state' in model_params:
        del model_params['random_state']

    tuned_xgb_model = XGBRegressor(**model_params, random_state=42, n_jobs=-1)
    tuned_xgb_model.fit(X, y_log)

    print("Final tuned XGBoost model trained successfully.")

    # Optionally save the model
    # dump(tuned_xgb_model, "tuned_xgboost_model.joblib")
    # print("Tuned XGBoost model saved as 'tuned_xgboost_model.joblib'")
else:
    print("Error: 'best_params' is not defined. Please run the Optuna optimization cell first.")